{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0310ea3f-39d4-4522-bfd2-303c8c76fcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import importlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import importlib\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "import src.models.mlp.mlp_cv_trainer_emb as cv\n",
    "import src.utils.optuna_visualizer as opv\n",
    "import src.utils.telegram as te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0483e9a2-ecc5-4078-a62a-31ab26efcca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "env_path = Path.cwd().parent / \".env\"\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "url = os.environ.get(\"OPTUNA_STORAGE_URL\")\n",
    "\n",
    "tr_df1 = pd.read_parquet(\"../artifacts/features/base/tr_df1.parquet\")\n",
    "test_df1 = pd.read_parquet(\"../artifacts/features/base/test_df1.parquet\").astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73f1cb74-d86d-4622-810a-ef5a4b6353a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1\n",
      "Epoch 1: Train Logloss = 0.88426, Val Logloss = 0.90463\n",
      "New best model saved at epoch 1, Logloss: 0.90463\n",
      "Epoch 2: Train Logloss = 0.26862, Val Logloss = 0.28598\n",
      "New best model saved at epoch 2, Logloss: 0.28598\n",
      "Epoch 3: Train Logloss = 0.24893, Val Logloss = 0.25984\n",
      "New best model saved at epoch 3, Logloss: 0.25984\n",
      "Epoch 4: Train Logloss = 0.23920, Val Logloss = 0.25278\n",
      "New best model saved at epoch 4, Logloss: 0.25278\n",
      "Epoch 5: Train Logloss = 0.22469, Val Logloss = 0.24430\n",
      "New best model saved at epoch 5, Logloss: 0.24430\n",
      "Epoch 6: Train Logloss = 0.23359, Val Logloss = 0.24609\n",
      "Epoch 7: Train Logloss = 0.24432, Val Logloss = 0.24949\n",
      "Epoch 8: Train Logloss = 0.21801, Val Logloss = 0.23306\n",
      "New best model saved at epoch 8, Logloss: 0.23306\n",
      "Epoch 9: Train Logloss = 0.21714, Val Logloss = 0.23230\n",
      "New best model saved at epoch 9, Logloss: 0.23230\n",
      "Epoch 10: Train Logloss = 0.22595, Val Logloss = 0.23385\n",
      "Epoch 11: Train Logloss = 0.20051, Val Logloss = 0.21205\n",
      "New best model saved at epoch 11, Logloss: 0.21205\n",
      "Epoch 12: Train Logloss = 0.18982, Val Logloss = 0.20035\n",
      "New best model saved at epoch 12, Logloss: 0.20035\n",
      "Epoch 13: Train Logloss = 0.18695, Val Logloss = 0.19993\n",
      "New best model saved at epoch 13, Logloss: 0.19993\n",
      "Epoch 14: Train Logloss = 0.20710, Val Logloss = 0.22263\n",
      "Epoch 15: Train Logloss = 0.18368, Val Logloss = 0.19444\n",
      "New best model saved at epoch 15, Logloss: 0.19444\n",
      "Epoch 16: Train Logloss = 0.18462, Val Logloss = 0.19403\n",
      "New best model saved at epoch 16, Logloss: 0.19403\n",
      "Epoch 17: Train Logloss = 0.18217, Val Logloss = 0.19653\n",
      "Epoch 18: Train Logloss = 0.17698, Val Logloss = 0.18907\n",
      "New best model saved at epoch 18, Logloss: 0.18907\n",
      "Epoch 19: Train Logloss = 0.18048, Val Logloss = 0.19733\n",
      "Epoch 20: Train Logloss = 0.18815, Val Logloss = 0.19810\n",
      "Epoch 21: Train Logloss = 0.17804, Val Logloss = 0.18870\n",
      "New best model saved at epoch 21, Logloss: 0.18870\n",
      "Epoch 22: Train Logloss = 0.17452, Val Logloss = 0.18707\n",
      "New best model saved at epoch 22, Logloss: 0.18707\n",
      "Epoch 23: Train Logloss = 0.17537, Val Logloss = 0.19170\n",
      "Epoch 24: Train Logloss = 0.17579, Val Logloss = 0.19158\n",
      "Epoch 25: Train Logloss = 0.17715, Val Logloss = 0.19346\n",
      "Epoch 26: Train Logloss = 0.17460, Val Logloss = 0.18662\n",
      "New best model saved at epoch 26, Logloss: 0.18662\n",
      "Epoch 27: Train Logloss = 0.17583, Val Logloss = 0.19017\n",
      "Epoch 28: Train Logloss = 0.17455, Val Logloss = 0.19283\n",
      "Epoch 29: Train Logloss = 0.17153, Val Logloss = 0.18488\n",
      "New best model saved at epoch 29, Logloss: 0.18488\n",
      "Epoch 30: Train Logloss = 0.17242, Val Logloss = 0.18728\n",
      "Epoch 31: Train Logloss = 0.17157, Val Logloss = 0.18628\n",
      "Epoch 32: Train Logloss = 0.17368, Val Logloss = 0.18486\n",
      "New best model saved at epoch 32, Logloss: 0.18486\n",
      "Epoch 33: Train Logloss = 0.17008, Val Logloss = 0.18641\n",
      "Epoch 34: Train Logloss = 0.16992, Val Logloss = 0.18678\n",
      "Epoch 35: Train Logloss = 0.17105, Val Logloss = 0.18476\n",
      "New best model saved at epoch 35, Logloss: 0.18476\n",
      "Epoch 36: Train Logloss = 0.16902, Val Logloss = 0.18536\n",
      "Epoch 37: Train Logloss = 0.17016, Val Logloss = 0.18584\n",
      "Epoch 38: Train Logloss = 0.16816, Val Logloss = 0.18315\n",
      "New best model saved at epoch 38, Logloss: 0.18315\n",
      "Epoch 39: Train Logloss = 0.17074, Val Logloss = 0.18687\n",
      "Epoch 40: Train Logloss = 0.16853, Val Logloss = 0.18391\n",
      "Epoch 41: Train Logloss = 0.16844, Val Logloss = 0.18555\n",
      "Epoch 42: Train Logloss = 0.16932, Val Logloss = 0.18503\n",
      "Epoch 43: Train Logloss = 0.16778, Val Logloss = 0.18435\n",
      "Epoch 44: Train Logloss = 0.16772, Val Logloss = 0.18332\n",
      "Epoch 45: Train Logloss = 0.16725, Val Logloss = 0.18321\n",
      "Epoch 46: Train Logloss = 0.16727, Val Logloss = 0.18356\n",
      "Epoch 47: Train Logloss = 0.16720, Val Logloss = 0.18358\n",
      "Epoch 48: Train Logloss = 0.16718, Val Logloss = 0.18344\n",
      "Epoch 49: Train Logloss = 0.16725, Val Logloss = 0.18381\n",
      "Epoch 50: Train Logloss = 0.16717, Val Logloss = 0.18341\n",
      "Epoch 51: Train Logloss = 0.16715, Val Logloss = 0.18352\n",
      "Epoch 52: Train Logloss = 0.16711, Val Logloss = 0.18359\n",
      "Epoch 53: Train Logloss = 0.16706, Val Logloss = 0.18361\n",
      "Epoch 54: Train Logloss = 0.16707, Val Logloss = 0.18366\n",
      "Epoch 55: Train Logloss = 0.16698, Val Logloss = 0.18376\n",
      "Epoch 56: Train Logloss = 0.16693, Val Logloss = 0.18383\n",
      "Epoch 57: Train Logloss = 0.16698, Val Logloss = 0.18355\n",
      "Epoch 58: Train Logloss = 0.16685, Val Logloss = 0.18400\n",
      "Epoch 59: Train Logloss = 0.16657, Val Logloss = 0.18308\n",
      "New best model saved at epoch 59, Logloss: 0.18308\n",
      "Epoch 60: Train Logloss = 0.16726, Val Logloss = 0.18447\n",
      "Epoch 61: Train Logloss = 0.16697, Val Logloss = 0.18252\n",
      "New best model saved at epoch 61, Logloss: 0.18252\n",
      "Epoch 62: Train Logloss = 0.16703, Val Logloss = 0.18392\n",
      "Epoch 63: Train Logloss = 0.16788, Val Logloss = 0.18389\n",
      "Epoch 64: Train Logloss = 0.16904, Val Logloss = 0.18840\n",
      "Epoch 65: Train Logloss = 0.16684, Val Logloss = 0.18471\n",
      "Epoch 66: Train Logloss = 0.16942, Val Logloss = 0.18669\n",
      "Epoch 67: Train Logloss = 0.16766, Val Logloss = 0.18455\n",
      "Epoch 68: Train Logloss = 0.16885, Val Logloss = 0.18397\n",
      "Epoch 69: Train Logloss = 0.16896, Val Logloss = 0.18319\n",
      "Epoch 70: Train Logloss = 0.16776, Val Logloss = 0.18429\n",
      "Epoch 71: Train Logloss = 0.17060, Val Logloss = 0.18670\n",
      "Epoch 72: Train Logloss = 0.16708, Val Logloss = 0.18423\n",
      "Epoch 73: Train Logloss = 0.17100, Val Logloss = 0.18955\n",
      "Epoch 74: Train Logloss = 0.17918, Val Logloss = 0.19069\n",
      "Epoch 75: Train Logloss = 0.17094, Val Logloss = 0.18709\n",
      "Epoch 76: Train Logloss = 0.16802, Val Logloss = 0.18311\n",
      "Epoch 77: Train Logloss = 0.18718, Val Logloss = 0.19815\n",
      "Epoch 78: Train Logloss = 0.17097, Val Logloss = 0.18839\n",
      "Epoch 79: Train Logloss = 0.17463, Val Logloss = 0.19067\n",
      "Epoch 80: Train Logloss = 0.17134, Val Logloss = 0.18855\n",
      "Epoch 81: Train Logloss = 0.17162, Val Logloss = 0.18874\n",
      "Epoch 82: Train Logloss = 0.17618, Val Logloss = 0.19195\n",
      "Early stopping at epoch 82\n",
      "Loading best model from epoch 61 with Logloss 0.18252\n",
      "Best Logloss: 0.18252\n",
      "Training time: 00:00:20\n",
      "\n",
      "Fold 2\n",
      "Epoch 1: Train Logloss = 0.59971, Val Logloss = 0.56789\n",
      "New best model saved at epoch 1, Logloss: 0.56789\n",
      "Epoch 2: Train Logloss = 0.40849, Val Logloss = 0.38816\n",
      "New best model saved at epoch 2, Logloss: 0.38816\n",
      "Epoch 3: Train Logloss = 0.28019, Val Logloss = 0.26713\n",
      "New best model saved at epoch 3, Logloss: 0.26713\n",
      "Epoch 4: Train Logloss = 0.23578, Val Logloss = 0.22972\n",
      "New best model saved at epoch 4, Logloss: 0.22972\n",
      "Epoch 5: Train Logloss = 0.22591, Val Logloss = 0.22374\n",
      "New best model saved at epoch 5, Logloss: 0.22374\n",
      "Epoch 6: Train Logloss = 0.22240, Val Logloss = 0.22064\n",
      "New best model saved at epoch 6, Logloss: 0.22064\n",
      "Epoch 7: Train Logloss = 0.23982, Val Logloss = 0.24388\n",
      "Epoch 8: Train Logloss = 0.21119, Val Logloss = 0.21478\n",
      "New best model saved at epoch 8, Logloss: 0.21478\n",
      "Epoch 9: Train Logloss = 0.19948, Val Logloss = 0.20563\n",
      "New best model saved at epoch 9, Logloss: 0.20563\n",
      "Epoch 10: Train Logloss = 0.20536, Val Logloss = 0.21765\n",
      "Epoch 11: Train Logloss = 0.19253, Val Logloss = 0.20341\n",
      "New best model saved at epoch 11, Logloss: 0.20341\n",
      "Epoch 12: Train Logloss = 0.19182, Val Logloss = 0.20222\n",
      "New best model saved at epoch 12, Logloss: 0.20222\n",
      "Epoch 13: Train Logloss = 0.18437, Val Logloss = 0.19452\n",
      "New best model saved at epoch 13, Logloss: 0.19452\n",
      "Epoch 14: Train Logloss = 0.19630, Val Logloss = 0.21162\n",
      "Epoch 15: Train Logloss = 0.18223, Val Logloss = 0.19521\n",
      "Epoch 16: Train Logloss = 0.18070, Val Logloss = 0.19388\n",
      "New best model saved at epoch 16, Logloss: 0.19388\n",
      "Epoch 17: Train Logloss = 0.18201, Val Logloss = 0.19924\n",
      "Epoch 18: Train Logloss = 0.19655, Val Logloss = 0.21470\n",
      "Epoch 19: Train Logloss = 0.17963, Val Logloss = 0.19644\n",
      "Epoch 20: Train Logloss = 0.17658, Val Logloss = 0.19114\n",
      "New best model saved at epoch 20, Logloss: 0.19114\n",
      "Epoch 21: Train Logloss = 0.18346, Val Logloss = 0.20097\n",
      "Epoch 22: Train Logloss = 0.17727, Val Logloss = 0.19093\n",
      "New best model saved at epoch 22, Logloss: 0.19093\n",
      "Epoch 23: Train Logloss = 0.18658, Val Logloss = 0.20795\n",
      "Epoch 24: Train Logloss = 0.18054, Val Logloss = 0.19909\n",
      "Epoch 25: Train Logloss = 0.17587, Val Logloss = 0.19499\n",
      "Epoch 26: Train Logloss = 0.17262, Val Logloss = 0.18889\n",
      "New best model saved at epoch 26, Logloss: 0.18889\n",
      "Epoch 27: Train Logloss = 0.17503, Val Logloss = 0.19169\n",
      "Epoch 28: Train Logloss = 0.17543, Val Logloss = 0.19231\n",
      "Epoch 29: Train Logloss = 0.17086, Val Logloss = 0.19064\n",
      "Epoch 30: Train Logloss = 0.16932, Val Logloss = 0.18925\n",
      "Epoch 31: Train Logloss = 0.16987, Val Logloss = 0.18880\n",
      "New best model saved at epoch 31, Logloss: 0.18880\n",
      "Epoch 32: Train Logloss = 0.17479, Val Logloss = 0.19718\n",
      "Epoch 33: Train Logloss = 0.17183, Val Logloss = 0.19043\n",
      "Epoch 34: Train Logloss = 0.16949, Val Logloss = 0.18802\n",
      "New best model saved at epoch 34, Logloss: 0.18802\n",
      "Epoch 35: Train Logloss = 0.16930, Val Logloss = 0.18911\n",
      "Epoch 36: Train Logloss = 0.16882, Val Logloss = 0.18833\n",
      "Epoch 37: Train Logloss = 0.17011, Val Logloss = 0.18945\n",
      "Epoch 38: Train Logloss = 0.16936, Val Logloss = 0.18947\n",
      "Epoch 39: Train Logloss = 0.16797, Val Logloss = 0.18793\n",
      "New best model saved at epoch 39, Logloss: 0.18793\n",
      "Epoch 40: Train Logloss = 0.17017, Val Logloss = 0.19335\n",
      "Epoch 41: Train Logloss = 0.16728, Val Logloss = 0.18742\n",
      "New best model saved at epoch 41, Logloss: 0.18742\n",
      "Epoch 42: Train Logloss = 0.16740, Val Logloss = 0.18750\n",
      "Epoch 43: Train Logloss = 0.16742, Val Logloss = 0.18847\n",
      "Epoch 44: Train Logloss = 0.16666, Val Logloss = 0.18755\n",
      "Epoch 45: Train Logloss = 0.16692, Val Logloss = 0.18798\n",
      "Epoch 46: Train Logloss = 0.16671, Val Logloss = 0.18779\n",
      "Epoch 47: Train Logloss = 0.16631, Val Logloss = 0.18723\n",
      "New best model saved at epoch 47, Logloss: 0.18723\n",
      "Epoch 48: Train Logloss = 0.16630, Val Logloss = 0.18750\n",
      "Epoch 49: Train Logloss = 0.16620, Val Logloss = 0.18742\n",
      "Epoch 50: Train Logloss = 0.16609, Val Logloss = 0.18732\n",
      "Epoch 51: Train Logloss = 0.16625, Val Logloss = 0.18756\n",
      "Epoch 52: Train Logloss = 0.16637, Val Logloss = 0.18767\n",
      "Epoch 53: Train Logloss = 0.16617, Val Logloss = 0.18739\n",
      "Epoch 54: Train Logloss = 0.16630, Val Logloss = 0.18799\n",
      "Epoch 55: Train Logloss = 0.16610, Val Logloss = 0.18780\n",
      "Epoch 56: Train Logloss = 0.16597, Val Logloss = 0.18763\n",
      "Epoch 57: Train Logloss = 0.16587, Val Logloss = 0.18734\n",
      "Epoch 58: Train Logloss = 0.16556, Val Logloss = 0.18645\n",
      "New best model saved at epoch 58, Logloss: 0.18645\n",
      "Epoch 59: Train Logloss = 0.16546, Val Logloss = 0.18671\n",
      "Epoch 60: Train Logloss = 0.16606, Val Logloss = 0.18798\n",
      "Epoch 61: Train Logloss = 0.16562, Val Logloss = 0.18759\n",
      "Epoch 62: Train Logloss = 0.16551, Val Logloss = 0.18688\n",
      "Epoch 63: Train Logloss = 0.16622, Val Logloss = 0.18773\n",
      "Epoch 64: Train Logloss = 0.16558, Val Logloss = 0.18676\n",
      "Epoch 65: Train Logloss = 0.16594, Val Logloss = 0.18751\n",
      "Epoch 66: Train Logloss = 0.16658, Val Logloss = 0.18630\n",
      "New best model saved at epoch 66, Logloss: 0.18630\n",
      "Epoch 67: Train Logloss = 0.16708, Val Logloss = 0.18860\n",
      "Epoch 68: Train Logloss = 0.16706, Val Logloss = 0.19198\n",
      "Epoch 69: Train Logloss = 0.16603, Val Logloss = 0.18829\n",
      "Epoch 70: Train Logloss = 0.16983, Val Logloss = 0.19271\n",
      "Epoch 71: Train Logloss = 0.16983, Val Logloss = 0.19108\n",
      "Epoch 72: Train Logloss = 0.16616, Val Logloss = 0.18808\n",
      "Epoch 73: Train Logloss = 0.16585, Val Logloss = 0.18728\n",
      "Epoch 74: Train Logloss = 0.16654, Val Logloss = 0.18610\n",
      "New best model saved at epoch 74, Logloss: 0.18610\n",
      "Epoch 75: Train Logloss = 0.16780, Val Logloss = 0.18683\n",
      "Epoch 76: Train Logloss = 0.16738, Val Logloss = 0.19225\n",
      "Epoch 77: Train Logloss = 0.16808, Val Logloss = 0.18908\n",
      "Epoch 78: Train Logloss = 0.16877, Val Logloss = 0.19321\n",
      "Epoch 79: Train Logloss = 0.17199, Val Logloss = 0.19174\n",
      "Epoch 80: Train Logloss = 0.16983, Val Logloss = 0.19051\n",
      "Epoch 81: Train Logloss = 0.17074, Val Logloss = 0.19057\n",
      "Epoch 82: Train Logloss = 0.17442, Val Logloss = 0.19612\n",
      "Epoch 83: Train Logloss = 0.17220, Val Logloss = 0.19224\n",
      "Epoch 84: Train Logloss = 0.16996, Val Logloss = 0.19398\n",
      "Epoch 85: Train Logloss = 0.17313, Val Logloss = 0.19184\n",
      "Epoch 86: Train Logloss = 0.17795, Val Logloss = 0.19938\n",
      "Epoch 87: Train Logloss = 0.17341, Val Logloss = 0.19560\n",
      "Epoch 88: Train Logloss = 0.16968, Val Logloss = 0.19323\n",
      "Epoch 89: Train Logloss = 0.17962, Val Logloss = 0.20574\n",
      "Epoch 90: Train Logloss = 0.17020, Val Logloss = 0.19379\n",
      "Epoch 91: Train Logloss = 0.17385, Val Logloss = 0.20011\n",
      "Epoch 92: Train Logloss = 0.19002, Val Logloss = 0.21597\n",
      "Epoch 93: Train Logloss = 0.17146, Val Logloss = 0.19822\n",
      "Epoch 94: Train Logloss = 0.17004, Val Logloss = 0.18912\n",
      "Epoch 95: Train Logloss = 0.16854, Val Logloss = 0.19147\n",
      "Early stopping at epoch 95\n",
      "Loading best model from epoch 74 with Logloss 0.18610\n",
      "Best Logloss: 0.18610\n",
      "Training time: 00:00:25\n",
      "\n",
      "Fold 3\n",
      "Epoch 1: Train Logloss = 0.62909, Val Logloss = 0.63190\n",
      "New best model saved at epoch 1, Logloss: 0.63190\n",
      "Epoch 2: Train Logloss = 0.25921, Val Logloss = 0.27065\n",
      "New best model saved at epoch 2, Logloss: 0.27065\n",
      "Epoch 3: Train Logloss = 0.24428, Val Logloss = 0.26242\n",
      "New best model saved at epoch 3, Logloss: 0.26242\n",
      "Epoch 4: Train Logloss = 0.23329, Val Logloss = 0.24782\n",
      "New best model saved at epoch 4, Logloss: 0.24782\n",
      "Epoch 5: Train Logloss = 0.22578, Val Logloss = 0.23712\n",
      "New best model saved at epoch 5, Logloss: 0.23712\n",
      "Epoch 6: Train Logloss = 0.24314, Val Logloss = 0.25430\n",
      "Epoch 7: Train Logloss = 0.22788, Val Logloss = 0.24470\n",
      "Epoch 8: Train Logloss = 0.23149, Val Logloss = 0.23926\n",
      "Epoch 9: Train Logloss = 0.19603, Val Logloss = 0.20185\n",
      "New best model saved at epoch 9, Logloss: 0.20185\n",
      "Epoch 10: Train Logloss = 0.20617, Val Logloss = 0.21505\n",
      "Epoch 11: Train Logloss = 0.19960, Val Logloss = 0.20888\n",
      "Epoch 12: Train Logloss = 0.20207, Val Logloss = 0.21102\n",
      "Epoch 13: Train Logloss = 0.19243, Val Logloss = 0.19709\n",
      "New best model saved at epoch 13, Logloss: 0.19709\n",
      "Epoch 14: Train Logloss = 0.18439, Val Logloss = 0.19144\n",
      "New best model saved at epoch 14, Logloss: 0.19144\n",
      "Epoch 15: Train Logloss = 0.19078, Val Logloss = 0.19847\n",
      "Epoch 16: Train Logloss = 0.18616, Val Logloss = 0.19519\n",
      "Epoch 17: Train Logloss = 0.18163, Val Logloss = 0.19268\n",
      "Epoch 18: Train Logloss = 0.18341, Val Logloss = 0.18775\n",
      "New best model saved at epoch 18, Logloss: 0.18775\n",
      "Epoch 19: Train Logloss = 0.18898, Val Logloss = 0.19337\n",
      "Epoch 20: Train Logloss = 0.18175, Val Logloss = 0.18787\n",
      "Epoch 21: Train Logloss = 0.18194, Val Logloss = 0.19404\n",
      "Epoch 22: Train Logloss = 0.18509, Val Logloss = 0.19373\n",
      "Epoch 23: Train Logloss = 0.17974, Val Logloss = 0.18842\n",
      "Epoch 24: Train Logloss = 0.18203, Val Logloss = 0.18988\n",
      "Epoch 25: Train Logloss = 0.18099, Val Logloss = 0.18980\n",
      "Epoch 26: Train Logloss = 0.18078, Val Logloss = 0.18843\n",
      "Epoch 27: Train Logloss = 0.18073, Val Logloss = 0.19113\n",
      "Epoch 28: Train Logloss = 0.17455, Val Logloss = 0.18041\n",
      "New best model saved at epoch 28, Logloss: 0.18041\n",
      "Epoch 29: Train Logloss = 0.17486, Val Logloss = 0.18589\n",
      "Epoch 30: Train Logloss = 0.17518, Val Logloss = 0.18161\n",
      "Epoch 31: Train Logloss = 0.18250, Val Logloss = 0.19521\n",
      "Epoch 32: Train Logloss = 0.17559, Val Logloss = 0.18215\n",
      "Epoch 33: Train Logloss = 0.17325, Val Logloss = 0.18201\n",
      "Epoch 34: Train Logloss = 0.17372, Val Logloss = 0.18383\n",
      "Epoch 35: Train Logloss = 0.17219, Val Logloss = 0.18091\n",
      "Epoch 36: Train Logloss = 0.17311, Val Logloss = 0.18118\n",
      "Epoch 37: Train Logloss = 0.17262, Val Logloss = 0.18294\n",
      "Epoch 38: Train Logloss = 0.17323, Val Logloss = 0.18433\n",
      "Epoch 39: Train Logloss = 0.17173, Val Logloss = 0.17955\n",
      "New best model saved at epoch 39, Logloss: 0.17955\n",
      "Epoch 40: Train Logloss = 0.17208, Val Logloss = 0.18264\n",
      "Epoch 41: Train Logloss = 0.17074, Val Logloss = 0.18016\n",
      "Epoch 42: Train Logloss = 0.16990, Val Logloss = 0.17901\n",
      "New best model saved at epoch 42, Logloss: 0.17901\n",
      "Epoch 43: Train Logloss = 0.16978, Val Logloss = 0.17899\n",
      "New best model saved at epoch 43, Logloss: 0.17899\n",
      "Epoch 44: Train Logloss = 0.16955, Val Logloss = 0.17965\n",
      "Epoch 45: Train Logloss = 0.16939, Val Logloss = 0.17944\n",
      "Epoch 46: Train Logloss = 0.16930, Val Logloss = 0.17942\n",
      "Epoch 47: Train Logloss = 0.16923, Val Logloss = 0.17929\n",
      "Epoch 48: Train Logloss = 0.16914, Val Logloss = 0.17903\n",
      "Epoch 49: Train Logloss = 0.16912, Val Logloss = 0.17917\n",
      "Epoch 50: Train Logloss = 0.16911, Val Logloss = 0.17919\n",
      "Epoch 51: Train Logloss = 0.16920, Val Logloss = 0.17926\n",
      "Epoch 52: Train Logloss = 0.16915, Val Logloss = 0.17937\n",
      "Epoch 53: Train Logloss = 0.16921, Val Logloss = 0.17943\n",
      "Epoch 54: Train Logloss = 0.16911, Val Logloss = 0.17936\n",
      "Epoch 55: Train Logloss = 0.16918, Val Logloss = 0.17924\n",
      "Epoch 56: Train Logloss = 0.16914, Val Logloss = 0.17899\n",
      "Epoch 57: Train Logloss = 0.16934, Val Logloss = 0.18021\n",
      "Epoch 58: Train Logloss = 0.16901, Val Logloss = 0.17952\n",
      "Epoch 59: Train Logloss = 0.16912, Val Logloss = 0.17852\n",
      "New best model saved at epoch 59, Logloss: 0.17852\n",
      "Epoch 60: Train Logloss = 0.16959, Val Logloss = 0.18095\n",
      "Epoch 61: Train Logloss = 0.16886, Val Logloss = 0.17962\n",
      "Epoch 62: Train Logloss = 0.16956, Val Logloss = 0.18209\n",
      "Epoch 63: Train Logloss = 0.16888, Val Logloss = 0.17850\n",
      "New best model saved at epoch 63, Logloss: 0.17850\n",
      "Epoch 64: Train Logloss = 0.16986, Val Logloss = 0.18126\n",
      "Epoch 65: Train Logloss = 0.16897, Val Logloss = 0.17797\n",
      "New best model saved at epoch 65, Logloss: 0.17797\n",
      "Epoch 66: Train Logloss = 0.16984, Val Logloss = 0.18085\n",
      "Epoch 67: Train Logloss = 0.17010, Val Logloss = 0.17772\n",
      "New best model saved at epoch 67, Logloss: 0.17772\n",
      "Epoch 68: Train Logloss = 0.17660, Val Logloss = 0.19001\n",
      "Epoch 69: Train Logloss = 0.17048, Val Logloss = 0.18137\n",
      "Epoch 70: Train Logloss = 0.17607, Val Logloss = 0.18879\n",
      "Epoch 71: Train Logloss = 0.17070, Val Logloss = 0.17914\n",
      "Epoch 72: Train Logloss = 0.17312, Val Logloss = 0.18612\n",
      "Epoch 73: Train Logloss = 0.17727, Val Logloss = 0.19140\n",
      "Epoch 74: Train Logloss = 0.17268, Val Logloss = 0.18073\n",
      "Epoch 75: Train Logloss = 0.17079, Val Logloss = 0.18286\n",
      "Epoch 76: Train Logloss = 0.17073, Val Logloss = 0.18147\n",
      "Epoch 77: Train Logloss = 0.17077, Val Logloss = 0.18152\n",
      "Epoch 78: Train Logloss = 0.17191, Val Logloss = 0.18000\n",
      "Epoch 79: Train Logloss = 0.17573, Val Logloss = 0.18556\n",
      "Epoch 80: Train Logloss = 0.19074, Val Logloss = 0.20024\n",
      "Epoch 81: Train Logloss = 0.17185, Val Logloss = 0.18105\n",
      "Epoch 82: Train Logloss = 0.17086, Val Logloss = 0.18085\n",
      "Epoch 83: Train Logloss = 0.17154, Val Logloss = 0.18379\n",
      "Epoch 84: Train Logloss = 0.17506, Val Logloss = 0.18339\n",
      "Epoch 85: Train Logloss = 0.17284, Val Logloss = 0.18299\n",
      "Epoch 86: Train Logloss = 0.17025, Val Logloss = 0.18155\n",
      "Epoch 87: Train Logloss = 0.17869, Val Logloss = 0.18909\n",
      "Epoch 88: Train Logloss = 0.17207, Val Logloss = 0.18428\n",
      "Early stopping at epoch 88\n",
      "Loading best model from epoch 67 with Logloss 0.17772\n",
      "Best Logloss: 0.17772\n",
      "Training time: 00:00:23\n",
      "\n",
      "Fold 4\n",
      "Epoch 1: Train Logloss = 0.31803, Val Logloss = 0.33530\n",
      "New best model saved at epoch 1, Logloss: 0.33530\n",
      "Epoch 2: Train Logloss = 0.27455, Val Logloss = 0.28269\n",
      "New best model saved at epoch 2, Logloss: 0.28269\n",
      "Epoch 3: Train Logloss = 0.25908, Val Logloss = 0.27054\n",
      "New best model saved at epoch 3, Logloss: 0.27054\n",
      "Epoch 4: Train Logloss = 0.23273, Val Logloss = 0.23999\n",
      "New best model saved at epoch 4, Logloss: 0.23999\n",
      "Epoch 5: Train Logloss = 0.22828, Val Logloss = 0.23307\n",
      "New best model saved at epoch 5, Logloss: 0.23307\n",
      "Epoch 6: Train Logloss = 0.23390, Val Logloss = 0.23418\n",
      "Epoch 7: Train Logloss = 0.22462, Val Logloss = 0.22699\n",
      "New best model saved at epoch 7, Logloss: 0.22699\n",
      "Epoch 8: Train Logloss = 0.21776, Val Logloss = 0.22243\n",
      "New best model saved at epoch 8, Logloss: 0.22243\n",
      "Epoch 9: Train Logloss = 0.23612, Val Logloss = 0.23803\n",
      "Epoch 10: Train Logloss = 0.19831, Val Logloss = 0.20201\n",
      "New best model saved at epoch 10, Logloss: 0.20201\n",
      "Epoch 11: Train Logloss = 0.19065, Val Logloss = 0.19305\n",
      "New best model saved at epoch 11, Logloss: 0.19305\n",
      "Epoch 12: Train Logloss = 0.19627, Val Logloss = 0.19786\n",
      "Epoch 13: Train Logloss = 0.19424, Val Logloss = 0.19679\n",
      "Epoch 14: Train Logloss = 0.19665, Val Logloss = 0.19985\n",
      "Epoch 15: Train Logloss = 0.19720, Val Logloss = 0.19965\n",
      "Epoch 16: Train Logloss = 0.18204, Val Logloss = 0.18794\n",
      "New best model saved at epoch 16, Logloss: 0.18794\n",
      "Epoch 17: Train Logloss = 0.19117, Val Logloss = 0.19391\n",
      "Epoch 18: Train Logloss = 0.18264, Val Logloss = 0.18630\n",
      "New best model saved at epoch 18, Logloss: 0.18630\n",
      "Epoch 19: Train Logloss = 0.17761, Val Logloss = 0.18245\n",
      "New best model saved at epoch 19, Logloss: 0.18245\n",
      "Epoch 20: Train Logloss = 0.18382, Val Logloss = 0.18859\n",
      "Epoch 21: Train Logloss = 0.18524, Val Logloss = 0.19044\n",
      "Epoch 22: Train Logloss = 0.18315, Val Logloss = 0.18705\n",
      "Epoch 23: Train Logloss = 0.17905, Val Logloss = 0.18283\n",
      "Epoch 24: Train Logloss = 0.17860, Val Logloss = 0.18417\n",
      "Epoch 25: Train Logloss = 0.17598, Val Logloss = 0.18348\n",
      "Epoch 26: Train Logloss = 0.19622, Val Logloss = 0.19885\n",
      "Epoch 27: Train Logloss = 0.17378, Val Logloss = 0.17842\n",
      "New best model saved at epoch 27, Logloss: 0.17842\n",
      "Epoch 28: Train Logloss = 0.17967, Val Logloss = 0.18648\n",
      "Epoch 29: Train Logloss = 0.17604, Val Logloss = 0.18266\n",
      "Epoch 30: Train Logloss = 0.17677, Val Logloss = 0.18344\n",
      "Epoch 31: Train Logloss = 0.17302, Val Logloss = 0.17949\n",
      "Epoch 32: Train Logloss = 0.17789, Val Logloss = 0.18514\n",
      "Epoch 33: Train Logloss = 0.17196, Val Logloss = 0.17855\n",
      "Epoch 34: Train Logloss = 0.17292, Val Logloss = 0.18077\n",
      "Epoch 35: Train Logloss = 0.17643, Val Logloss = 0.18401\n",
      "Epoch 36: Train Logloss = 0.17086, Val Logloss = 0.17981\n",
      "Epoch 37: Train Logloss = 0.17095, Val Logloss = 0.18040\n",
      "Epoch 38: Train Logloss = 0.17132, Val Logloss = 0.18084\n",
      "Epoch 39: Train Logloss = 0.17052, Val Logloss = 0.18032\n",
      "Epoch 40: Train Logloss = 0.17150, Val Logloss = 0.18169\n",
      "Epoch 41: Train Logloss = 0.16990, Val Logloss = 0.17959\n",
      "Epoch 42: Train Logloss = 0.17139, Val Logloss = 0.18082\n",
      "Epoch 43: Train Logloss = 0.16921, Val Logloss = 0.17871\n",
      "Epoch 44: Train Logloss = 0.16902, Val Logloss = 0.17846\n",
      "Epoch 45: Train Logloss = 0.16892, Val Logloss = 0.17815\n",
      "New best model saved at epoch 45, Logloss: 0.17815\n",
      "Epoch 46: Train Logloss = 0.16902, Val Logloss = 0.17823\n",
      "Epoch 47: Train Logloss = 0.16882, Val Logloss = 0.17816\n",
      "Epoch 48: Train Logloss = 0.16890, Val Logloss = 0.17823\n",
      "Epoch 49: Train Logloss = 0.16886, Val Logloss = 0.17817\n",
      "Epoch 50: Train Logloss = 0.16873, Val Logloss = 0.17832\n",
      "Epoch 51: Train Logloss = 0.16860, Val Logloss = 0.17821\n",
      "Epoch 52: Train Logloss = 0.16849, Val Logloss = 0.17816\n",
      "Epoch 53: Train Logloss = 0.16856, Val Logloss = 0.17813\n",
      "New best model saved at epoch 53, Logloss: 0.17813\n",
      "Epoch 54: Train Logloss = 0.16852, Val Logloss = 0.17810\n",
      "New best model saved at epoch 54, Logloss: 0.17810\n",
      "Epoch 55: Train Logloss = 0.16842, Val Logloss = 0.17807\n",
      "New best model saved at epoch 55, Logloss: 0.17807\n",
      "Epoch 56: Train Logloss = 0.16844, Val Logloss = 0.17753\n",
      "New best model saved at epoch 56, Logloss: 0.17753\n",
      "Epoch 57: Train Logloss = 0.16853, Val Logloss = 0.17728\n",
      "New best model saved at epoch 57, Logloss: 0.17728\n",
      "Epoch 58: Train Logloss = 0.16831, Val Logloss = 0.17757\n",
      "Epoch 59: Train Logloss = 0.16811, Val Logloss = 0.17710\n",
      "New best model saved at epoch 59, Logloss: 0.17710\n",
      "Epoch 60: Train Logloss = 0.16830, Val Logloss = 0.17771\n",
      "Epoch 61: Train Logloss = 0.16856, Val Logloss = 0.17857\n",
      "Epoch 62: Train Logloss = 0.16871, Val Logloss = 0.17761\n",
      "Epoch 63: Train Logloss = 0.16892, Val Logloss = 0.17872\n",
      "Epoch 64: Train Logloss = 0.16899, Val Logloss = 0.17903\n",
      "Epoch 65: Train Logloss = 0.16995, Val Logloss = 0.18087\n",
      "Epoch 66: Train Logloss = 0.16835, Val Logloss = 0.17679\n",
      "New best model saved at epoch 66, Logloss: 0.17679\n",
      "Epoch 67: Train Logloss = 0.16835, Val Logloss = 0.17635\n",
      "New best model saved at epoch 67, Logloss: 0.17635\n",
      "Epoch 68: Train Logloss = 0.16986, Val Logloss = 0.17990\n",
      "Epoch 69: Train Logloss = 0.16825, Val Logloss = 0.17878\n",
      "Epoch 70: Train Logloss = 0.16927, Val Logloss = 0.18046\n",
      "Epoch 71: Train Logloss = 0.16866, Val Logloss = 0.17972\n",
      "Epoch 72: Train Logloss = 0.17149, Val Logloss = 0.17995\n",
      "Epoch 73: Train Logloss = 0.16956, Val Logloss = 0.17760\n",
      "Epoch 74: Train Logloss = 0.17793, Val Logloss = 0.18527\n",
      "Epoch 75: Train Logloss = 0.17539, Val Logloss = 0.18409\n",
      "Epoch 76: Train Logloss = 0.17213, Val Logloss = 0.18193\n",
      "Epoch 77: Train Logloss = 0.17197, Val Logloss = 0.18332\n",
      "Epoch 78: Train Logloss = 0.17114, Val Logloss = 0.18432\n",
      "Epoch 79: Train Logloss = 0.17139, Val Logloss = 0.18259\n",
      "Epoch 80: Train Logloss = 0.17604, Val Logloss = 0.18855\n",
      "Epoch 81: Train Logloss = 0.17866, Val Logloss = 0.18805\n",
      "Epoch 82: Train Logloss = 0.17437, Val Logloss = 0.18196\n",
      "Epoch 83: Train Logloss = 0.18403, Val Logloss = 0.19258\n",
      "Epoch 84: Train Logloss = 0.17642, Val Logloss = 0.18829\n",
      "Epoch 85: Train Logloss = 0.17026, Val Logloss = 0.18276\n",
      "Epoch 86: Train Logloss = 0.17178, Val Logloss = 0.18179\n",
      "Epoch 87: Train Logloss = 0.17506, Val Logloss = 0.18686\n",
      "Epoch 88: Train Logloss = 0.17580, Val Logloss = 0.18396\n",
      "Early stopping at epoch 88\n",
      "Loading best model from epoch 67 with Logloss 0.17635\n",
      "Best Logloss: 0.17635\n",
      "Training time: 00:00:23\n",
      "\n",
      "Fold 5\n",
      "Epoch 1: Train Logloss = 0.45713, Val Logloss = 0.44860\n",
      "New best model saved at epoch 1, Logloss: 0.44860\n",
      "Epoch 2: Train Logloss = 0.31349, Val Logloss = 0.31015\n",
      "New best model saved at epoch 2, Logloss: 0.31015\n",
      "Epoch 3: Train Logloss = 0.25052, Val Logloss = 0.23644\n",
      "New best model saved at epoch 3, Logloss: 0.23644\n",
      "Epoch 4: Train Logloss = 0.23901, Val Logloss = 0.22936\n",
      "New best model saved at epoch 4, Logloss: 0.22936\n",
      "Epoch 5: Train Logloss = 0.24074, Val Logloss = 0.23148\n",
      "Epoch 6: Train Logloss = 0.23707, Val Logloss = 0.23483\n",
      "Epoch 7: Train Logloss = 0.23516, Val Logloss = 0.23196\n",
      "Epoch 8: Train Logloss = 0.21222, Val Logloss = 0.21087\n",
      "New best model saved at epoch 8, Logloss: 0.21087\n",
      "Epoch 9: Train Logloss = 0.22541, Val Logloss = 0.22139\n",
      "Epoch 10: Train Logloss = 0.21184, Val Logloss = 0.20919\n",
      "New best model saved at epoch 10, Logloss: 0.20919\n",
      "Epoch 11: Train Logloss = 0.19587, Val Logloss = 0.19622\n",
      "New best model saved at epoch 11, Logloss: 0.19622\n",
      "Epoch 12: Train Logloss = 0.18929, Val Logloss = 0.18796\n",
      "New best model saved at epoch 12, Logloss: 0.18796\n",
      "Epoch 13: Train Logloss = 0.19258, Val Logloss = 0.19112\n",
      "Epoch 14: Train Logloss = 0.19050, Val Logloss = 0.18710\n",
      "New best model saved at epoch 14, Logloss: 0.18710\n",
      "Epoch 15: Train Logloss = 0.19201, Val Logloss = 0.18964\n",
      "Epoch 16: Train Logloss = 0.18378, Val Logloss = 0.18749\n",
      "Epoch 17: Train Logloss = 0.19516, Val Logloss = 0.19533\n",
      "Epoch 18: Train Logloss = 0.18730, Val Logloss = 0.18199\n",
      "New best model saved at epoch 18, Logloss: 0.18199\n",
      "Epoch 19: Train Logloss = 0.17895, Val Logloss = 0.17717\n",
      "New best model saved at epoch 19, Logloss: 0.17717\n",
      "Epoch 20: Train Logloss = 0.18260, Val Logloss = 0.18124\n",
      "Epoch 21: Train Logloss = 0.19453, Val Logloss = 0.19365\n",
      "Epoch 22: Train Logloss = 0.18065, Val Logloss = 0.18152\n",
      "Epoch 23: Train Logloss = 0.18506, Val Logloss = 0.18360\n",
      "Epoch 24: Train Logloss = 0.17992, Val Logloss = 0.18189\n",
      "Epoch 25: Train Logloss = 0.18119, Val Logloss = 0.18152\n",
      "Epoch 26: Train Logloss = 0.17771, Val Logloss = 0.17706\n",
      "New best model saved at epoch 26, Logloss: 0.17706\n",
      "Epoch 27: Train Logloss = 0.18347, Val Logloss = 0.18469\n",
      "Epoch 28: Train Logloss = 0.17614, Val Logloss = 0.17724\n",
      "Epoch 29: Train Logloss = 0.18162, Val Logloss = 0.18126\n",
      "Epoch 30: Train Logloss = 0.17926, Val Logloss = 0.18198\n",
      "Epoch 31: Train Logloss = 0.17600, Val Logloss = 0.17602\n",
      "New best model saved at epoch 31, Logloss: 0.17602\n",
      "Epoch 32: Train Logloss = 0.17490, Val Logloss = 0.17657\n",
      "Epoch 33: Train Logloss = 0.17877, Val Logloss = 0.17939\n",
      "Epoch 34: Train Logloss = 0.17616, Val Logloss = 0.17636\n",
      "Epoch 35: Train Logloss = 0.17323, Val Logloss = 0.17393\n",
      "New best model saved at epoch 35, Logloss: 0.17393\n",
      "Epoch 36: Train Logloss = 0.17410, Val Logloss = 0.17513\n",
      "Epoch 37: Train Logloss = 0.17360, Val Logloss = 0.17402\n",
      "Epoch 38: Train Logloss = 0.17249, Val Logloss = 0.17429\n",
      "Epoch 39: Train Logloss = 0.17348, Val Logloss = 0.17456\n",
      "Epoch 40: Train Logloss = 0.17128, Val Logloss = 0.17245\n",
      "New best model saved at epoch 40, Logloss: 0.17245\n",
      "Epoch 41: Train Logloss = 0.17150, Val Logloss = 0.17289\n",
      "Epoch 42: Train Logloss = 0.17101, Val Logloss = 0.17212\n",
      "New best model saved at epoch 42, Logloss: 0.17212\n",
      "Epoch 43: Train Logloss = 0.17134, Val Logloss = 0.17246\n",
      "Epoch 44: Train Logloss = 0.17063, Val Logloss = 0.17222\n",
      "Epoch 45: Train Logloss = 0.17039, Val Logloss = 0.17219\n",
      "Epoch 46: Train Logloss = 0.17030, Val Logloss = 0.17208\n",
      "New best model saved at epoch 46, Logloss: 0.17208\n",
      "Epoch 47: Train Logloss = 0.17009, Val Logloss = 0.17226\n",
      "Epoch 48: Train Logloss = 0.17008, Val Logloss = 0.17216\n",
      "Epoch 49: Train Logloss = 0.17003, Val Logloss = 0.17221\n",
      "Epoch 50: Train Logloss = 0.17017, Val Logloss = 0.17224\n",
      "Epoch 51: Train Logloss = 0.17009, Val Logloss = 0.17219\n",
      "Epoch 52: Train Logloss = 0.16997, Val Logloss = 0.17217\n",
      "Epoch 53: Train Logloss = 0.16993, Val Logloss = 0.17214\n",
      "Epoch 54: Train Logloss = 0.17015, Val Logloss = 0.17210\n",
      "Epoch 55: Train Logloss = 0.16999, Val Logloss = 0.17196\n",
      "New best model saved at epoch 55, Logloss: 0.17196\n",
      "Epoch 56: Train Logloss = 0.16984, Val Logloss = 0.17191\n",
      "New best model saved at epoch 56, Logloss: 0.17191\n",
      "Epoch 57: Train Logloss = 0.17015, Val Logloss = 0.17210\n",
      "Epoch 58: Train Logloss = 0.16973, Val Logloss = 0.17240\n",
      "Epoch 59: Train Logloss = 0.17115, Val Logloss = 0.17273\n",
      "Epoch 60: Train Logloss = 0.16978, Val Logloss = 0.17197\n",
      "Epoch 61: Train Logloss = 0.17076, Val Logloss = 0.17266\n",
      "Epoch 62: Train Logloss = 0.16972, Val Logloss = 0.17171\n",
      "New best model saved at epoch 62, Logloss: 0.17171\n",
      "Epoch 63: Train Logloss = 0.17135, Val Logloss = 0.17357\n",
      "Epoch 64: Train Logloss = 0.17027, Val Logloss = 0.17249\n",
      "Epoch 65: Train Logloss = 0.16942, Val Logloss = 0.17228\n",
      "Epoch 66: Train Logloss = 0.17072, Val Logloss = 0.17324\n",
      "Epoch 67: Train Logloss = 0.16978, Val Logloss = 0.17172\n",
      "Epoch 68: Train Logloss = 0.17012, Val Logloss = 0.17316\n",
      "Epoch 69: Train Logloss = 0.17378, Val Logloss = 0.17514\n",
      "Epoch 70: Train Logloss = 0.16965, Val Logloss = 0.17356\n",
      "Epoch 71: Train Logloss = 0.17123, Val Logloss = 0.17362\n",
      "Epoch 72: Train Logloss = 0.17682, Val Logloss = 0.18103\n",
      "Epoch 73: Train Logloss = 0.17613, Val Logloss = 0.17591\n",
      "Epoch 74: Train Logloss = 0.17483, Val Logloss = 0.17465\n",
      "Epoch 75: Train Logloss = 0.17445, Val Logloss = 0.17690\n",
      "Epoch 76: Train Logloss = 0.17305, Val Logloss = 0.17440\n",
      "Epoch 77: Train Logloss = 0.17405, Val Logloss = 0.17633\n",
      "Epoch 78: Train Logloss = 0.17346, Val Logloss = 0.17564\n",
      "Epoch 79: Train Logloss = 0.17230, Val Logloss = 0.17727\n",
      "Epoch 80: Train Logloss = 0.17402, Val Logloss = 0.17750\n",
      "Epoch 81: Train Logloss = 0.17693, Val Logloss = 0.18028\n",
      "Epoch 82: Train Logloss = 0.17301, Val Logloss = 0.17714\n",
      "Epoch 83: Train Logloss = 0.17264, Val Logloss = 0.17749\n",
      "Early stopping at epoch 83\n",
      "Loading best model from epoch 62 with Logloss 0.17171\n",
      "Best Logloss: 0.17171\n",
      "Training time: 00:00:20\n",
      "\n",
      "=== CV 結果 ===\n",
      "Fold scores: [0.1825202959572579, 0.18609974209268096, 0.17771588920503126, 0.17634936909106597, 0.17171153155863186]\n",
      "Mean: 0.17888, Std: 0.00499\n",
      "OOF score: 0.17888\n",
      "Avg best epoch: 66.2\n",
      "Best epochs: \n",
      "[61, 74, 67, 67, 62]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([9.15673503e-04, 3.73956258e-03, 6.85121340e-04, ...,\n",
       "        2.75696954e-03, 4.03698388e-04, 8.50866795e-01]),\n",
       " array([1.42337151e-03, 3.27580243e-01, 2.26052708e-04, 3.19067163e-04,\n",
       "        2.73101769e-02, 6.22890186e-03, 5.52518845e-01, 2.46622616e-01,\n",
       "        1.08662968e-02, 3.49120879e-01, 3.79057776e-04, 7.81822547e-02,\n",
       "        3.21429747e-01, 4.44068821e-04, 1.29718797e-01, 8.82878110e-02,\n",
       "        3.10097898e-04, 8.55326448e-03, 2.65442786e-01, 1.30404659e-01,\n",
       "        2.49567965e-01, 5.60587537e-01, 5.68085106e-04, 5.65503869e-04,\n",
       "        8.73938977e-04, 4.76800566e-04, 5.44994557e-01, 3.09601301e-04,\n",
       "        2.13090777e-01, 8.71951652e-01, 3.29512054e-01, 4.69302651e-04,\n",
       "        3.84412671e-04, 5.61242378e-03, 5.44069374e-01, 4.65076731e-03,\n",
       "        4.96767065e-04, 2.05411182e-03, 1.51623426e-03, 1.15769383e-03,\n",
       "        2.57704881e-01, 2.33702815e-02, 8.43752216e-04, 7.11627828e-04,\n",
       "        1.93843091e-01, 1.30509262e-01, 5.24134765e-04, 4.52502177e-04,\n",
       "        6.58788395e-01, 3.56207317e-01, 3.98852493e-04, 8.90614453e-04,\n",
       "        7.46059191e-01, 2.89042440e-02, 9.32413503e-04, 3.09034172e-04,\n",
       "        7.31760979e-02, 2.87257368e-04, 3.37052019e-04, 5.82336239e-03,\n",
       "        2.03578281e-01, 3.60483065e-04, 6.04328373e-04, 3.84079093e-01,\n",
       "        5.28371037e-04, 7.86876568e-04, 2.81514617e-04, 9.79614004e-02,\n",
       "        3.78828152e-04, 3.51860141e-04, 2.86386392e-04, 7.63868046e-01,\n",
       "        7.05697597e-04, 3.33005369e-01, 5.39410103e-04, 2.22293878e-04,\n",
       "        4.35496279e-04, 8.89648334e-04, 4.00097077e-04, 2.41458458e-03,\n",
       "        6.07731231e-03, 2.06383353e-02, 1.00812712e-01, 2.88070587e-04,\n",
       "        5.25394914e-04, 2.47894711e-04, 9.97153580e-02, 5.73124671e-01,\n",
       "        1.75275332e-01, 1.95930862e-01, 1.20375978e-03, 1.00200793e-01,\n",
       "        6.37773147e-02, 1.27229576e-01, 4.78273743e-01, 8.12464905e-01,\n",
       "        5.57596999e-04, 1.28246461e-02, 4.70233371e-04, 3.87860829e-04,\n",
       "        5.35066919e-03, 1.69302720e-01, 2.20779667e-04, 5.08350166e-04,\n",
       "        2.63028595e-04, 8.63672036e-04, 2.01861175e-04, 4.29096661e-04,\n",
       "        7.24295795e-01, 7.23795965e-04, 2.27456205e-03, 3.87125230e-01,\n",
       "        9.49196960e-04, 2.90069173e-04, 3.13972825e-01, 3.93757406e-03,\n",
       "        8.66078166e-03, 1.98686159e-01, 5.06979460e-04, 7.28972185e-01,\n",
       "        1.97709012e-01, 2.77507669e-04, 1.85988370e-01, 6.13824173e-04,\n",
       "        1.26385541e-03, 2.23535942e-03, 4.06358280e-04, 1.40916814e-01,\n",
       "        6.08856525e-04, 4.76846236e-01, 1.05357930e-01, 1.27216462e-02,\n",
       "        5.84645543e-04, 5.39544856e-04, 1.48675065e-02, 6.26810026e-01,\n",
       "        5.10176614e-02, 2.93454831e-04, 1.51872233e-02, 3.39934131e-04,\n",
       "        2.01307235e-03, 3.96276312e-04, 4.26775444e-01, 2.07521475e-03,\n",
       "        1.96722127e-02, 1.10741701e-03, 2.34768694e-02, 2.77270176e-04,\n",
       "        1.39354037e-03, 2.12803036e-01, 5.15974513e-03, 8.72022271e-01,\n",
       "        8.83659016e-04, 1.98420632e-01, 5.11073053e-02, 6.78995383e-01,\n",
       "        8.56810937e-03, 4.66978783e-04, 1.78586761e-03, 1.02884965e-01,\n",
       "        7.22697802e-02, 4.92209952e-03, 7.69877050e-04, 1.48969421e-01,\n",
       "        1.94508201e-01, 1.81304212e-03, 1.64579076e-01, 2.44939487e-04,\n",
       "        7.89108232e-02, 6.99477241e-04, 2.38031932e-03, 3.84821051e-01,\n",
       "        2.84197999e-04, 4.90564942e-01, 7.67867398e-01, 1.02865691e-01,\n",
       "        3.72633466e-04, 1.01312400e-01, 6.06101664e-04, 6.16573995e-01,\n",
       "        7.82400537e-01, 2.63182953e-02, 3.84566412e-04, 1.30058969e-03,\n",
       "        3.00704484e-04, 2.06654624e-03, 1.24416023e-01, 9.37878044e-04,\n",
       "        9.86540387e-04, 2.42959155e-04, 3.72057223e-01, 5.37649036e-04,\n",
       "        1.37587102e-04, 7.31586502e-04, 3.82774984e-03, 2.32331760e-04,\n",
       "        7.67939043e-01, 1.84361346e-03, 7.86223747e-02, 2.90325391e-01,\n",
       "        1.39283720e-02, 2.11528246e-04, 1.57480096e-02, 3.70083299e-02,\n",
       "        7.52906883e-01, 2.52238047e-04, 3.21126555e-04, 2.11502169e-04,\n",
       "        1.25393305e-03, 7.15639163e-04, 1.95931898e-04, 3.36308766e-04,\n",
       "        4.23350252e-04, 2.32530863e-03, 1.07026171e-03, 4.85080737e-03,\n",
       "        3.55318870e-04, 1.88486060e-03, 1.50263814e-02, 8.20128155e-01,\n",
       "        8.06056726e-01, 1.01954194e-03, 4.39832438e-04, 8.21477330e-01,\n",
       "        1.28600905e-02, 6.07550336e-04, 3.92520456e-03, 1.06832571e-03,\n",
       "        1.23182209e-02, 5.40213346e-01, 3.39985746e-01, 1.94536939e-01,\n",
       "        2.01826532e-04, 1.03073425e-03, 4.96153188e-01, 1.96291935e-01,\n",
       "        3.58933044e-04, 6.66569054e-01, 2.50120446e-01, 5.07343543e-01,\n",
       "        6.82646322e-01, 3.73306805e-01, 5.38953475e-04, 3.24296546e-03,\n",
       "        1.36201521e-02, 1.47563141e-02, 4.34324250e-03, 1.22671240e-03,\n",
       "        4.13796878e-01, 2.00938886e-01, 6.31721853e-04, 1.75240597e-01,\n",
       "        3.36665541e-01, 1.92517182e-01, 6.84971521e-03, 9.43595171e-03,\n",
       "        8.18811917e-01, 6.69027655e-04, 4.30613518e-04, 5.87904358e-01,\n",
       "        1.25367472e-01, 2.61200475e-02, 2.81802274e-04, 3.64597293e-04,\n",
       "        6.02494122e-04, 2.12940788e-03, 3.26789659e-02, 4.63495584e-04,\n",
       "        3.26189169e-01, 1.26072784e-03, 1.02730294e-03, 2.61741481e-04,\n",
       "        9.92504463e-02, 6.01396686e-04, 1.27049787e-03, 3.34913691e-04,\n",
       "        3.67686895e-04, 6.07052013e-02, 7.46584558e-01, 3.69960000e-04,\n",
       "        1.24476007e-03, 1.05805005e-01, 1.47861904e-03, 3.84331977e-01,\n",
       "        5.88819961e-04, 1.71471328e-01, 8.96442088e-04, 2.83366922e-01,\n",
       "        9.32093913e-04, 9.80640412e-04, 2.96040706e-04, 1.00759534e-03,\n",
       "        2.59899424e-04, 1.94899097e-02, 5.09055785e-04, 2.93177547e-04,\n",
       "        5.80083538e-04, 8.25501180e-01, 1.45762433e-01, 2.38506595e-04,\n",
       "        3.99221070e-04, 3.44213101e-04, 9.05319620e-03, 1.39603713e-01,\n",
       "        2.73975525e-03, 4.38141689e-02, 6.12585485e-04, 2.07029005e-03,\n",
       "        6.09758461e-04, 1.13478783e-03, 5.04699122e-04, 4.35273341e-04,\n",
       "        4.10397840e-01, 4.38318893e-04, 5.73750281e-01, 6.32600985e-02,\n",
       "        2.98873498e-03, 3.03771964e-04, 1.59112480e-03, 9.83963875e-04,\n",
       "        3.32404423e-01, 1.49747634e-01, 1.82207015e-03, 2.04703711e-02,\n",
       "        1.02519990e-03, 3.49665532e-04, 4.96906112e-04, 2.71705966e-04,\n",
       "        7.27700937e-04, 2.14179122e-04, 1.53156028e-03, 7.92127597e-01,\n",
       "        2.14457512e-04, 7.92752515e-04, 5.82796129e-04, 2.35139413e-04,\n",
       "        3.59557364e-04, 1.03800098e-03, 2.77077465e-04, 2.90853184e-04,\n",
       "        1.93969350e-02, 1.74888727e-03, 1.68832259e-03, 1.05868189e-03,\n",
       "        1.00590658e-01, 4.46222365e-04, 2.03134182e-01, 3.76756084e-01,\n",
       "        5.15251339e-04, 2.44385111e-01, 3.61051864e-04, 2.42342456e-04,\n",
       "        7.47427321e-04, 4.10427921e-04, 7.74852361e-04, 3.75841686e-04,\n",
       "        8.88386503e-02, 5.61026623e-04, 5.83276343e-01, 2.22581905e-01,\n",
       "        3.70767558e-01, 1.93178681e-01, 4.81101623e-04, 3.61130647e-03,\n",
       "        2.13686450e-04, 1.38561791e-03, 3.91391991e-04, 9.16241843e-04,\n",
       "        1.23101848e-03, 2.13171420e-01, 4.97537870e-03, 2.46979832e-03,\n",
       "        1.90104659e-03, 1.61443328e-04, 7.53998884e-04, 1.47381786e-01,\n",
       "        6.98985011e-04, 6.43420517e-02, 3.25600266e-01, 1.61688384e-01,\n",
       "        1.18922500e-03, 7.60849967e-04, 1.57704264e-03, 2.98815811e-01,\n",
       "        1.25240150e-01, 9.55902511e-04, 8.20936430e-01, 7.90320020e-03,\n",
       "        8.72580451e-04, 2.60691968e-04, 1.83406967e-01, 4.15171784e-01,\n",
       "        5.34610060e-04, 9.66737136e-02, 9.02033388e-04, 4.07684137e-04,\n",
       "        2.63672417e-01, 7.35203281e-02, 2.36912081e-02, 6.11213996e-04,\n",
       "        6.60042145e-04, 1.94292415e-03, 2.83600911e-04, 6.56908739e-01,\n",
       "        6.75958506e-04, 2.96988833e-01, 3.21186399e-01, 5.25513594e-04,\n",
       "        3.46166128e-04, 2.21447367e-04, 4.57934185e-04, 1.31223124e-02,\n",
       "        1.48461205e-02, 3.87730098e-01, 2.69149639e-02, 8.75409353e-01,\n",
       "        4.09256685e-01, 4.68675458e-01, 2.70852551e-01, 2.01600549e-01,\n",
       "        8.01454806e-01, 2.85533283e-04, 2.00827774e-02, 2.04066254e-04,\n",
       "        1.52124192e-04, 2.96958402e-04, 4.18438709e-01, 9.13773134e-02,\n",
       "        7.65878204e-04, 4.13634562e-01, 1.85320869e-03, 2.24845735e-03,\n",
       "        2.28719903e-02, 2.78738906e-04, 1.55217333e-03, 5.80508611e-04,\n",
       "        8.76257801e-01, 4.12228427e-04, 6.30228356e-03, 1.52155100e-03,\n",
       "        1.74598973e-02, 3.50645880e-04, 3.07104701e-01, 3.43054482e-03,\n",
       "        4.38429380e-04, 4.81355800e-03, 6.19175315e-01, 5.43116161e-04,\n",
       "        7.91182482e-01, 1.12839172e-03, 9.23291638e-02, 1.10946079e-01,\n",
       "        1.74315486e-04, 4.33698010e-01, 5.31985480e-01, 1.40805237e-03,\n",
       "        2.13084102e-03, 5.85273129e-04, 4.44119534e-04, 1.50171055e-03,\n",
       "        6.83924429e-02, 2.17831463e-02, 2.13926589e-02, 4.86406786e-03,\n",
       "        6.12780917e-04, 8.41039193e-01, 5.23928303e-04, 2.51554826e-04,\n",
       "        6.44418737e-04, 2.49634280e-03, 2.96659526e-04, 8.22142343e-04,\n",
       "        1.99039161e-04, 3.34906722e-03, 2.26129065e-04, 8.80944300e-01,\n",
       "        2.53716205e-03, 3.07896292e-01, 8.27336580e-02, 6.09838992e-02,\n",
       "        3.15825075e-01, 7.39138606e-03, 2.43037473e-03, 3.00317633e-01,\n",
       "        1.05434605e-02, 4.33255249e-04, 3.34280991e-04, 1.51311225e-03,\n",
       "        4.01563157e-04, 2.55778857e-04, 8.23289811e-04, 3.86228973e-01,\n",
       "        2.73050659e-03, 8.72345013e-04, 2.78216542e-04, 6.86783122e-04,\n",
       "        5.06790570e-04, 3.80588978e-04, 3.73280901e-01, 2.23681853e-03,\n",
       "        1.00978147e-03, 4.81103014e-04, 7.54037854e-04, 9.59117233e-04,\n",
       "        1.71033589e-03, 2.52271476e-04, 1.14802300e-02, 3.68589920e-01,\n",
       "        4.40853798e-01, 6.33398008e-01, 3.52156218e-04, 8.56274652e-01,\n",
       "        1.85423288e-03, 3.24924209e-04, 2.12502133e-04, 5.32750931e-04,\n",
       "        4.14143133e-01, 9.61354882e-02, 3.54802233e-04, 8.38723533e-02,\n",
       "        1.16071719e-02, 1.62814438e-01, 5.19461924e-01, 9.70321484e-02,\n",
       "        8.73286128e-01, 3.76064997e-04, 5.24380904e-01, 1.68688297e-03,\n",
       "        4.22312338e-02, 3.81452456e-04, 1.23174216e-03, 4.59233049e-04,\n",
       "        6.31165855e-02, 5.56318872e-03, 1.31899463e-02, 4.43131065e-01,\n",
       "        4.74931322e-02, 1.68801791e-03, 3.83241312e-03, 7.00869000e-01,\n",
       "        1.05824167e-01, 1.26334125e-03, 4.07722453e-04, 2.30471132e-04,\n",
       "        4.33792947e-03, 4.82624513e-03, 4.50452053e-04, 9.04460922e-03,\n",
       "        5.09041583e-04, 3.52833857e-04, 3.77724755e-03, 3.84231773e-03,\n",
       "        2.07769179e-03, 8.79104421e-02, 6.66427636e-01, 3.08369945e-03,\n",
       "        1.64370582e-04, 3.23361988e-04, 4.92159271e-01, 6.27076571e-04,\n",
       "        7.13597007e-02, 3.43270891e-04, 8.75424728e-04, 2.13705355e-03,\n",
       "        2.31252199e-01, 2.49362130e-02, 2.17442344e-03, 7.49105104e-04,\n",
       "        2.40633349e-04, 8.46587300e-01, 4.19989744e-01, 2.27004150e-04,\n",
       "        4.69365400e-01, 2.19886433e-04, 8.11424971e-01, 1.09902149e-03,\n",
       "        2.42655468e-01, 8.20611522e-02, 8.06421340e-01, 1.08570691e-01,\n",
       "        3.18492032e-04, 5.82725089e-04, 1.71821761e-03, 1.25307026e-03,\n",
       "        1.75627689e-03, 3.42073229e-01, 1.85971842e-04, 7.98526096e-01,\n",
       "        3.13716618e-04, 3.92317295e-01, 1.34680094e-03, 4.95200005e-04,\n",
       "        4.11418378e-01, 6.57806659e-01, 7.75602926e-04, 6.04134989e-01,\n",
       "        1.68115690e-03, 6.13288629e-01, 8.98603199e-04, 5.39010228e-03,\n",
       "        4.45300113e-02, 2.39493842e-03, 8.82944209e-04, 3.99576442e-04,\n",
       "        8.87979865e-03, 5.03705311e-04, 4.47583771e-01, 1.29029930e-03,\n",
       "        2.55991885e-04, 2.85456496e-01, 3.63333081e-04, 4.03916788e-01,\n",
       "        1.35810240e-01, 8.92186258e-04, 5.69844019e-01, 2.76800635e-04,\n",
       "        4.50651109e-01, 2.36505960e-04, 5.14566759e-03, 5.92798069e-02,\n",
       "        3.35715312e-04, 4.10341739e-04, 7.37054694e-01, 6.46296277e-04,\n",
       "        8.93265742e-04, 3.65316556e-04, 3.08167094e-01, 2.09479472e-04,\n",
       "        2.56122684e-01, 3.23199838e-01, 1.28626088e-03, 2.83869945e-03,\n",
       "        3.21123903e-03, 2.05862948e-03, 3.11293936e-04, 3.47721410e-01,\n",
       "        6.47740418e-04, 2.08891788e-04, 1.49362761e-01, 5.47309113e-01,\n",
       "        6.89213383e-01, 2.59184033e-01, 3.03537343e-04, 1.41510130e-03,\n",
       "        3.45461079e-04, 3.65709333e-04, 2.59264922e-04, 4.68263138e-04,\n",
       "        1.61547613e-01, 2.55355194e-01, 8.67513740e-01, 2.41108274e-04,\n",
       "        5.24362773e-03, 8.54924216e-04, 3.28158564e-03, 6.33378699e-03,\n",
       "        3.57213619e-04, 1.46948623e-02, 3.85094387e-03, 1.91424761e-02,\n",
       "        5.36173568e-03, 1.46687197e-03, 1.20873501e-03, 9.68871012e-02,\n",
       "        2.16435137e-01, 1.27167071e-01, 8.81718518e-04, 1.38388942e-01,\n",
       "        3.24770808e-03, 2.82711114e-04, 7.41168283e-02, 1.94549119e-01,\n",
       "        4.48275532e-04, 1.05529430e-02, 1.01030872e-03, 3.59892123e-04,\n",
       "        2.45585686e-01, 1.19177246e-01, 3.11201750e-04, 7.86846828e-01,\n",
       "        8.55956411e-01, 5.88600757e-03, 3.65696035e-04, 4.43835044e-04,\n",
       "        7.97134258e-02, 2.68660372e-04, 4.02931839e-01, 2.76942176e-01,\n",
       "        3.06253010e-04, 9.68205296e-03, 6.03920926e-04, 5.23365216e-04,\n",
       "        2.28131565e-04, 5.37763262e-01, 1.46124688e-01, 4.47672522e-01,\n",
       "        6.26205152e-04, 6.58767176e-01, 3.57697115e-04, 5.87663418e-04,\n",
       "        7.81481974e-02, 1.32475810e-01, 5.19013822e-01, 2.12720493e-04,\n",
       "        8.05037379e-01, 2.13163518e-04, 2.52494990e-04, 2.43678867e-04,\n",
       "        8.20680853e-04, 3.08034086e-04, 2.88968583e-04, 3.24737674e-01,\n",
       "        1.11908651e-02, 2.22247967e-01, 6.93852675e-01, 1.01544675e-01,\n",
       "        2.40422050e-02, 2.77259338e-03, 2.73726895e-04, 3.21042906e-03,\n",
       "        2.65860185e-04, 4.54073292e-04, 1.47140118e-03, 5.13721304e-03,\n",
       "        4.01391941e-01, 4.20885670e-01, 3.67109739e-04, 3.13989024e-04,\n",
       "        4.64219534e-01, 2.05637552e-04, 8.94187589e-04, 2.24330270e-01,\n",
       "        2.75681254e-02, 3.96808481e-01, 7.44196004e-04, 8.56656003e-01,\n",
       "        6.52015388e-01, 2.06591251e-01, 1.00590048e-02, 8.49743292e-02,\n",
       "        3.19571377e-04, 8.11531825e-04, 1.72923380e-01, 2.70862656e-04,\n",
       "        7.99188161e-01, 1.71421617e-01, 6.51113904e-04, 6.55385113e-04,\n",
       "        2.18571333e-04, 3.64836754e-04, 3.44082210e-04, 7.14085897e-04,\n",
       "        6.36309211e-04, 4.68301162e-04, 2.63258903e-03, 6.49816198e-03,\n",
       "        2.68631559e-04, 2.03137484e-01, 8.52655888e-01, 4.47534740e-02,\n",
       "        3.76770768e-04, 2.17604777e-03, 3.48338106e-04, 1.35195075e-01,\n",
       "        2.34558732e-04, 7.12786050e-04, 4.37609636e-04, 3.42074852e-04,\n",
       "        2.87494599e-04, 1.85491164e-04, 2.51831318e-04, 2.46597936e-03,\n",
       "        3.23751621e-02, 7.83812992e-02, 1.71314302e-01, 3.50180839e-04,\n",
       "        3.51669741e-01, 2.07080773e-01, 3.08585957e-01, 3.45671645e-01,\n",
       "        3.55012342e-03, 8.61937165e-01, 6.01358855e-01, 4.43628341e-02,\n",
       "        2.36278554e-04, 4.63255718e-03, 5.28291452e-01, 2.24247217e-04,\n",
       "        7.63999589e-04, 3.54057268e-04, 4.45467781e-04, 4.61711705e-04,\n",
       "        3.42990659e-04, 2.07489848e-04, 2.93126282e-01, 3.64443916e-04,\n",
       "        6.23201119e-04, 2.20347986e-01, 6.18667698e-01, 3.28614414e-01,\n",
       "        5.81463601e-04, 4.72966791e-04, 4.92896186e-04, 2.27697205e-04,\n",
       "        2.60872990e-01, 1.72386421e-03, 7.27342465e-04, 2.40241384e-04,\n",
       "        6.28709706e-04, 8.25212955e-01, 3.96605354e-04, 5.08811194e-01,\n",
       "        3.17565235e-04, 3.92882684e-03, 7.70961845e-01, 3.01775851e-04,\n",
       "        7.11158526e-01, 4.99507406e-04, 1.46159545e-03, 2.71790478e-01,\n",
       "        1.13229884e-03, 5.49921624e-02, 1.62774143e-03, 5.92359371e-02,\n",
       "        7.96870895e-02, 2.40316894e-03, 3.32054505e-04, 7.45559554e-03,\n",
       "        7.47270310e-01, 6.72535372e-01, 4.51854782e-04, 7.05883920e-04,\n",
       "        4.41922629e-04, 4.19809306e-01, 4.77881846e-04, 2.25384870e-01,\n",
       "        6.38726908e-03, 1.09492213e-03, 6.46743726e-04, 2.29349785e-04,\n",
       "        2.59289639e-03, 2.32247580e-02, 3.87461530e-04, 4.94031918e-01,\n",
       "        4.35752869e-01, 1.13116401e-01, 7.69856751e-01, 3.75721341e-03,\n",
       "        1.31034399e-03, 6.55037642e-04, 8.17946531e-04, 2.98990358e-03,\n",
       "        4.32787940e-04, 2.54710149e-03, 1.04910994e-02, 5.03307410e-02,\n",
       "        2.14745264e-04, 8.60066190e-02, 9.16934424e-04, 2.58151926e-03,\n",
       "        2.36161783e-01, 4.69058245e-01, 6.13103634e-01, 7.23281484e-03,\n",
       "        2.67850320e-04, 5.55761741e-04, 6.66825590e-04, 1.54579738e-01,\n",
       "        6.42561703e-04, 1.03842339e-01, 1.37605983e-03, 3.87896266e-04,\n",
       "        8.52428555e-02, 6.24370913e-04, 6.10281415e-02, 1.43034986e-01,\n",
       "        2.11905149e-01, 6.65118801e-03, 2.01615085e-03, 1.63542463e-02,\n",
       "        3.21910477e-01, 9.81810503e-04, 2.83300625e-03, 1.79824716e-01,\n",
       "        1.36265990e-01, 6.80041350e-03, 6.86570299e-01, 6.11979060e-04,\n",
       "        1.47404318e-01, 7.91507661e-03, 2.77945588e-04, 4.73389341e-04,\n",
       "        2.87852079e-01, 2.82646218e-04, 2.28014305e-04, 4.08055341e-01,\n",
       "        3.99247281e-02, 5.13423181e-01, 1.23617088e-03, 6.94598473e-04,\n",
       "        8.71093893e-01, 1.03959080e-01, 1.14639394e-03, 5.20042449e-01,\n",
       "        5.66216841e-04, 2.24379891e-01, 1.32052504e-01, 3.29679513e-04,\n",
       "        5.17459423e-04, 4.44883657e-01, 6.95490301e-02, 3.82096184e-04,\n",
       "        2.31149467e-03, 7.01120591e-01, 6.68310077e-04, 3.96742715e-04,\n",
       "        2.57930419e-01, 4.07828481e-04, 1.36005001e-02, 1.07391548e-01,\n",
       "        2.93751960e-03, 2.65169659e-03, 2.91566171e-03, 8.66699463e-04,\n",
       "        3.17931251e-04, 2.26840479e-03, 2.22148595e-04, 1.26000104e-03,\n",
       "        2.29216659e-04, 7.49621773e-01, 2.65835011e-01, 9.67385219e-03,\n",
       "        5.18244725e-01, 3.22812633e-04, 2.58784926e-01, 6.63375901e-03,\n",
       "        7.30042084e-04, 4.50533442e-02, 2.34241670e-01, 4.43556845e-02,\n",
       "        4.73723371e-02, 5.13132219e-03, 1.49244367e-01, 1.64674070e-04,\n",
       "        4.48923477e-04, 1.46880255e-03, 2.09533122e-03, 2.37490132e-03,\n",
       "        6.18768524e-04, 1.33640007e-03, 3.37269866e-01, 4.42442610e-03,\n",
       "        2.72258019e-01, 4.22758918e-04, 1.49516452e-03, 5.31837463e-02,\n",
       "        1.95928584e-04, 2.68679845e-01, 3.54753382e-04, 4.82813299e-01,\n",
       "        1.33055209e-03, 2.13855553e-01, 9.67307517e-04, 2.64828676e-01,\n",
       "        1.49883424e-03, 2.19655284e-01, 4.88474065e-01, 8.56455171e-01,\n",
       "        2.90298250e-03, 1.39050651e-03, 3.66616070e-01, 1.02187296e-03,\n",
       "        4.28488207e-01, 1.91344973e-03, 9.58217293e-02, 7.61244071e-01,\n",
       "        1.71461672e-01, 3.52245683e-04, 8.85291234e-02, 1.60395113e-01,\n",
       "        2.68729753e-04, 5.21631219e-04, 7.33867958e-02, 1.84421893e-03,\n",
       "        2.69209393e-04, 1.04859404e-03, 4.63044326e-04, 5.78454807e-03,\n",
       "        1.80895898e-01, 7.54070916e-04, 6.46630704e-01, 8.70639277e-01,\n",
       "        2.91316491e-04, 5.23128456e-03, 3.08216101e-01, 8.15597379e-01,\n",
       "        4.85394627e-04, 3.23793679e-01, 1.55283755e-01, 7.59508344e-04]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create OOF and test predictions\n",
    "importlib.reload(cv)\n",
    "tr_df1 = tr_df1[:10000]\n",
    "test_df1 = test_df1[:1000]\n",
    "params = {\n",
    "    \"num_layers\": 3,\n",
    "    \"hidden_dim1\": 352,\n",
    "    \"hidden_dim2\": 256,\n",
    "    \"hidden_dim3\": 224,\n",
    "    \"hidden_dim4\": 128,\n",
    "    \"batch_size\": 896,\n",
    "    \"lr\": 0.016867490361457304,\n",
    "    \"eta_min\": 3.9793716038378347e-05,\n",
    "    \"dropout_rate\": 0.35,\n",
    "    \"activation\": \"GELU\"\n",
    "}\n",
    "trainer = cv.MLPCVTrainer(**params)\n",
    "trainer.fit(tr_df1, test_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d9c3744-be45-4f44-829a-693a1a346065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.named_parameters at 0x7f02dda5c040>\n"
     ]
    }
   ],
   "source": [
    "model = trainer.fold_models[0].model\n",
    "print(model.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff542d6a-0856-44a5-b24d-b36daf2aef87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding_layers.job.weight\n",
      "embedding_layers.marital.weight\n",
      "embedding_layers.education.weight\n",
      "embedding_layers.default.weight\n",
      "embedding_layers.housing.weight\n",
      "embedding_layers.loan.weight\n",
      "embedding_layers.contact.weight\n",
      "embedding_layers.month.weight\n",
      "embedding_layers.poutcome.weight\n",
      "net.0.weight\n",
      "net.0.bias\n",
      "net.1.weight\n",
      "net.1.bias\n",
      "net.4.weight\n",
      "net.4.bias\n",
      "net.5.weight\n",
      "net.5.bias\n",
      "net.8.weight\n",
      "net.8.bias\n",
      "net.9.weight\n",
      "net.9.bias\n",
      "net.12.weight\n",
      "net.12.bias\n",
      "net.13.weight\n",
      "net.13.bias\n",
      "net.16.weight\n",
      "net.16.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-2.2.0",
   "language": "python",
   "name": "torch22"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
